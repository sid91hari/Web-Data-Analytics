{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import urllib.request\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restuarant URLs with random 50 Pure-Live and 50 Non-Live Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for restaurant name and URLs (Input file)\n",
    "restaurants = pd.read_excel(\"restaurants_2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "# Store Restaurant URLs and names in lists\n",
    "rest_links = restaurants['Restaurant_URL'].tolist()\n",
    "rest_names = restaurants['Restaurant_Name'].tolist()\n",
    "print(len(rest_links),len(rest_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape reviews for 100 restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rest in range(len(rest_links)):\n",
    "    # Create empty lists for review, author, rating and restaurant\n",
    "    review = []\n",
    "    author = []\n",
    "    rating = []\n",
    "    restaurant = []\n",
    "\n",
    "    # Store Main URL\n",
    "    yelp_restaurant_url = rest_links[rest]\n",
    "    # Set sleep time\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Open Main URL\n",
    "    html = urllib.request.urlopen(yelp_restaurant_url).read().decode('utf-8')\n",
    "\n",
    "    # Create a list to store all pages\n",
    "    final_links = [yelp_restaurant_url]\n",
    "\n",
    "    # Iterate over all next links and store them in a list \n",
    "    while html.find(\"<link rel=\\\"next\\\" href=\\\"\") != -1:\n",
    "        start_url = html.find(\"<link rel=\\\"next\\\" href=\\\"\")\n",
    "        end_url = html.find(\"<script type=\\\"application/ld+json\\\">\")\n",
    "        new_link = html[start_url+23:end_url-11]\n",
    "        final_links.append(new_link)\n",
    "        time.sleep(3)\n",
    "        html = urllib.request.urlopen(new_link).read().decode('utf-8')\n",
    "\n",
    "    # Iterate over all links\n",
    "    for link in final_links:\n",
    "        time.sleep(3)\n",
    "        html = urllib.request.urlopen(link).read().decode('utf-8')\n",
    "        b = html.find(\"{\\\"reviewRating\\\"\")\n",
    "        c = html.find(\"\\\"@context\\\"\")\n",
    "        html2 = html[b+1:c-2]\n",
    "\n",
    "        # Iterate to store all reviews and rating\n",
    "        while html2.find(\"\\\"reviewRating\\\"\") != -1:\n",
    "            desc_index = html2.find(\"\\\"description\\\"\")\n",
    "            author_index = html2.find(\"\\\"author\\\"\")\n",
    "            desc = html2[desc_index+16:author_index-3]\n",
    "            desc = desc.replace(\"\\\\n\\\\n\",\" \")\n",
    "            desc = desc.replace(\"\\\\n\",\" \")\n",
    "            desc = desc.replace(\"\\\\u0026\",\"&\")\n",
    "            review.append(desc)\n",
    "            rating_index = html2.find(\"\\\"ratingValue\\\"\")\n",
    "            rate_end = html2.find(\"\\\"datePublished\\\"\")\n",
    "            r = html2[rating_index+15:rate_end-3]\n",
    "            rating.append(r)\n",
    "\n",
    "            remaining = html2[author_index:] \n",
    "            author_index = remaining.find(\"\\\"author\\\"\")\n",
    "            if remaining.find(\"\\\"reviewRating\\\"\") != -1:\n",
    "                end = remaining.find(\"\\\"reviewRating\\\"\")\n",
    "                author.append(remaining[author_index+11:end-5])\n",
    "            else:\n",
    "                end = remaining.find(\"}]\")\n",
    "                author.append(remaining[author_index+11:end-1])\n",
    "            html2 = remaining[end:]\n",
    "\n",
    "    # Store Restaurant name in a list\n",
    "    for i in range(len(review)):\n",
    "        restaurant.append(rest_names[rest])\n",
    "\n",
    "    # Write each restaurant data to csv and merge all during sentiment analysis    \n",
    "    outputdf = pd.DataFrame(list(zip(restaurant,rating, review)),columns=[\"Name\", \"Rating\",\"Review\"])\n",
    "    file_name = rest_names[rest] + '.csv'\n",
    "    outputdf.to_csv(file_name, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
